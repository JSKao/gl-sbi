import jax
import jax.numpy as jnp
from flax.training import train_state
import optax # éœ€è¦ pip install optax

from gl_jax import GLSolverJAX, SimConfig
from model import NREClassifier # å‡è¨­ä½ çš„æª”æ¡ˆå« model.py
from dataclasses import dataclass

# --- 1. è¨­å®šå€ ---
@dataclass
class TrainConfig:
    # æ•¸æ“šç”Ÿæˆ
    batch_size: int = 8
    n_samples: int = 1000 # ç¸½å…±è¨“ç·´å¹¾å€‹ Step (ç‚ºäº†æ¼”ç¤ºå…ˆè¨­å°ä¸€é»)
    
    # è¨“ç·´åƒæ•¸
    learning_rate: float = 1e-3
    seed: int = 42

# --- 2. æ¨¡æ“¬å™¨ (Data Factory) ---
# é€™æ˜¯æˆ‘å€‘ä¹‹å‰å¯«å¥½çš„ï¼Œä¿æŒä¸è®Š
def simulator(key):
    # 1. Split key
    key_eta, key_B, key_sim = jax.random.split(key, 3)
    
    # 2. Prior Sampling
    eta = jax.random.uniform(key_eta, minval=0.0, maxval=1.5)
    B = jax.random.uniform(key_B, minval=0.0, maxval=0.02)
    
    # 3. Config & Solver
    config = SimConfig(eta=eta, B=B, N=32)
    solver = GLSolverJAX(config)
    
    # 4. Evolution
    psi1_init, psi2_init = GLSolverJAX.initialize_state(config, key_sim)
    psi1_final, psi2_final = solver.evolve(psi1_init, psi2_init, 100)
    
    # 5. Feature Extraction (Density)
    rho1 = jnp.abs(psi1_final) ** 2
    rho2 = jnp.abs(psi2_final) ** 2
    density = jnp.stack([rho1, rho2], axis=-1) # æ”¹æˆ channel last: (128, 128, 2) ç¬¦åˆ CNN ç¿’æ…£
    
    # 6. Return
    params = jnp.array([eta, B])
    return params, density

# --- 3. è¨“ç·´ç‹€æ…‹ç®¡ç† (Deep Skill: Flax Pattern) ---
def create_train_state(rng, learning_rate):
    """åˆå§‹åŒ–æ¨¡å‹åƒæ•¸èˆ‡å„ªåŒ–å™¨"""
    model = NREClassifier()
    
    # å‡æ•¸æ“šç”¨æ–¼åˆå§‹åŒ–å½¢ç‹€
    dummy_x = jnp.ones((1, 128, 128, 2))
    dummy_theta = jnp.ones((1, 2))
    
    # åˆå§‹åŒ–åƒæ•¸
    variables = model.init(rng, dummy_x, dummy_theta)
    
    # è¨­å®šå„ªåŒ–å™¨ (Adam)
    tx = optax.adam(learning_rate)
    
    # å»ºç«‹ TrainState (å®ƒæœƒå¹«æˆ‘å€‘ä¿ç®¡ params å’Œ opt_state)
    return train_state.TrainState.create(
        apply_fn=model.apply,
        params=variables['params'],
        tx=tx,
    )

# --- 4. æå¤±å‡½æ•¸èˆ‡æ›´æ–°æ­¥ (The Brain) ---
@jax.jit
def train_step(state, batch_x, batch_theta, key_shuffle):
    """
    åŸ·è¡Œä¸€æ­¥è¨“ç·´ï¼š
    1. æº–å‚™æ­£æ¨£æœ¬ (Joint)
    2. è£½é€ è² æ¨£æœ¬ (Marginal, by shuffling)
    3. è¨ˆç®— Loss
    4. åå‘å‚³æ’­ (Backprop)
    """
    
    # A. æº–å‚™æ•¸æ“š ------------------------------------------------
    # æ­£æ¨£æœ¬: (x, theta) -> Label 1
    pos_x = batch_x
    pos_theta = batch_theta
    pos_labels = jnp.ones((batch_x.shape[0], 1))
    
    # è² æ¨£æœ¬: (x, theta_shuffled) -> Label 0
    # Deep Skill: é€é "å¤§é¢¨å¹" (Roll) å¿«é€Ÿè£½é€ ä¸åŒ¹é…çš„åƒæ•¸
    # é€™æ¨£ x é‚„æ˜¯åˆæ³•çš„ xï¼Œtheta é‚„æ˜¯åˆæ³•çš„ thetaï¼Œä½†é…åœ¨ä¸€èµ·å°±æ˜¯éŒ¯çš„
    neg_x = batch_x
    neg_theta = jnp.roll(batch_theta, shift=1, axis=0) 
    neg_labels = jnp.zeros((batch_x.shape[0], 1))
    
    # åˆä½µæ•¸æ“š (Concatenate)
    train_x = jnp.concatenate([pos_x, neg_x], axis=0)
    train_theta = jnp.concatenate([pos_theta, neg_theta], axis=0)
    train_labels = jnp.concatenate([pos_labels, neg_labels], axis=0)
    
    # B. å®šç¾© Loss Function (Closure) ---------------------------
    def loss_fn(params):
        # Forward pass (è¨ˆç®— Logits)
        logits = state.apply_fn({'params': params}, train_x, train_theta)
        
        # è¨ˆç®— Binary Cross Entropy
        # ä½¿ç”¨ optax çš„ç©©å®šç‰ˆæœ¬ (å…§å« Sigmoid)
        loss = optax.sigmoid_binary_cross_entropy(logits, train_labels).mean()
        return loss

    # C. è¨ˆç®—æ¢¯åº¦èˆ‡æ›´æ–° -----------------------------------------
    # jax.value_and_grad æœƒåŒæ™‚ç®—å‡º loss å€¼å’Œ gradients
    loss, grads = jax.value_and_grad(loss_fn)(state.params)
    
    # æ›´æ–°æ¨¡å‹åƒæ•¸
    new_state = state.apply_gradients(grads=grads)
    
    return new_state, loss

# --- 5. ä¸»ç¨‹å¼ ---
if __name__ == "__main__":
    cfg = TrainConfig()
    print(f"ğŸš€ é–‹å§‹è¨“ç·´ NRE æ¨¡å‹ | Batch: {cfg.batch_size} | Steps: {cfg.n_samples}")
    
    # 1. æº–å‚™éš¨æ©Ÿé‘°åŒ™
    master_key = jax.random.PRNGKey(cfg.seed)
    key_init, key_train = jax.random.split(master_key)
    
    # 2. åˆå§‹åŒ– AI (TrainState)
    state = create_train_state(key_init, cfg.learning_rate)
    print("âœ… æ¨¡å‹èˆ‡å„ªåŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # 3. æº–å‚™æ‰¹æ¬¡æ¨¡æ“¬å™¨ (vmap)
    batched_simulator = jax.vmap(simulator)
    
    # 4. è¨“ç·´è¿´åœˆ (Training Loop)
    # åœ¨é€™è£¡æˆ‘å€‘ã€Œé‚Šè·‘æ¨¡æ“¬ã€é‚Šè¨“ç·´ã€(Online Training)
    # é€™æ¯”å…ˆå­˜ç¡¬ç¢Ÿå†è®€å–æ›´çœç©ºé–“ï¼Œä¸”åˆ©ç”¨ GPU é«˜ä½µç™¼å„ªå‹¢
    
    key_current = key_train
    
    for step in range(cfg.n_samples):
        # A. æ›´æ–°éš¨æ©Ÿé‘°åŒ™
        key_current, key_sim_batch, key_shuffle = jax.random.split(key_current, 3)
        sim_keys = jax.random.split(key_sim_batch, cfg.batch_size)
        
        # B. ç”¢ç”Ÿæ•¸æ“š (Physics Engine)
        # é€™è£¡æœƒå‘¼å« GPU é€²è¡Œ 32 å€‹å®‡å®™çš„å¹³è¡Œæ¼”åŒ–
        batch_theta, batch_x = batched_simulator(sim_keys)
        
        # C. è¨“ç·´ä¸€æ­¥ (AI Brain)
        state, loss = train_step(state, batch_x, batch_theta, key_shuffle)
        
        # D. ç›£æ§
        if step % 10 == 0:
            # ç°¡å–®çš„é€²åº¦æ¢
            print(f"Step {step:04d} | Loss: {loss:.4f} | (Physics + AI running...)")

    print("\nğŸ‰ è¨“ç·´å®Œæˆï¼")
    
    # --- 6. é©—æ”¶æ™‚åˆ»ï¼šAI çœŸçš„æ‡‚ç‰©ç†å—ï¼Ÿ ---
    print("\nğŸ” é–‹å§‹é€²è¡Œæ¨è«–æ¸¬è©¦ (Inference Demo)...")
    import matplotlib.pyplot as plt
    import numpy as np

    # A. ç”¢ç”Ÿä¸€å€‹ "çœŸå¯¦è§€æ¸¬" (Ground Truth)
    # æˆ‘å€‘è¨­å®šä¸€å€‹å·²çŸ¥çš„ç‰©ç†æƒ…æ³
    true_eta = 0.8
    true_B = 0.01
    
    # ç‚ºäº†å…¬å¹³ï¼Œæˆ‘å€‘ç”¨ä¸€æŠŠå…¨æ–°çš„é‘°åŒ™ä¾†æ¨¡æ“¬
    key_infer, key_sim = jax.random.split(jax.random.PRNGKey(999))
    
    # è·‘ä¸€æ¬¡æ¨¡æ“¬æ‹¿åˆ° "è§€æ¸¬åœ–ç‰‡" (Observation)
    print(f"1. æ­£åœ¨ç”ŸæˆçœŸå¯¦è§€æ¸¬æ•¸æ“š (True eta={true_eta}, True B={true_B})...")
    config = SimConfig(eta=true_eta, B=true_B, N=32) # ä¿æŒ N=32 èˆ‡è¨“ç·´ä¸€è‡´
    solver = GLSolverJAX(config)
    psi1, psi2 = GLSolverJAX.initialize_state(config, key_sim)
    p1_f, p2_f = solver.evolve(psi1, psi2, 100) # ä¿æŒ steps=100
    
    # è½‰æˆå¯†åº¦åœ– (128, 128, 2)
    obs_img = jnp.stack([jnp.abs(p1_f)**2, jnp.abs(p2_f)**2], axis=-1)
    # å¢åŠ  Batch ç¶­åº¦ -> (1, 128, 128, 2)
    obs_img_batch = jnp.expand_dims(obs_img, axis=0)

    # B. è®“ AI é€²è¡Œ "æƒæå¼æ¨è«–"
    # æˆ‘å€‘å›ºå®š Bï¼Œæƒæ eta å¾ 0.0 åˆ° 1.5ï¼Œçœ‹ AI è¦ºå¾—å“ªå€‹æœ€åƒ
    print("2. AI æ­£åœ¨æƒæåƒæ•¸ç©ºé–“ï¼Œå°‹æ‰¾æœ€å¯èƒ½çš„ eta...")
    
    test_etas = jnp.linspace(0.0, 1.5, 100) # æ¸¬è©¦ 100 å€‹ä¸åŒçš„ eta
    scores = []
    
    for test_eta in test_etas:
        # å»ºç«‹æ¸¬è©¦åƒæ•¸å° (test_eta, true_B)
        # æ³¨æ„ï¼šæˆ‘å€‘ä½œå¼Šå‘Šè¨´å®ƒ B æ˜¯å°çš„ï¼Œåªè€ƒå®ƒ eta (ç‚ºäº† demo ç°¡å–®)
        theta_test = jnp.array([[test_eta, true_B]])
        
        # è®“ AI æ‰“åˆ†æ•¸ (Forward Pass)
        # ä¸éœ€è¦ç®—æ¢¯åº¦ï¼Œç›´æ¥ apply
        logit = state.apply_fn({'params': state.params}, obs_img_batch, theta_test)
        
        # è½‰æˆæ©Ÿç‡ (Sigmoid)
        prob = jax.nn.sigmoid(logit)
        scores.append(prob[0, 0])
    
    scores = np.array(scores)

    # C. ç•«åœ–é©—è­‰
    print("3. ç¹ªè£½çµæœ...")
    plt.figure(figsize=(10, 6))
    plt.plot(test_etas, scores, label='AI Confidence', color='blue', linewidth=2)
    plt.axvline(x=true_eta, color='red', linestyle='--', label=f'True Eta ({true_eta})')
    plt.title("Neural Ratio Estimation: Inference Result", fontsize=14)
    plt.xlabel("Eta (Coupling Strength)", fontsize=12)
    plt.ylabel("AI Probability Score (Posterior Proxy)", fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å„²å­˜åœ–ç‰‡
    plt.savefig("inference_result.png")
    print(f"âœ… æ¨è«–å®Œæˆï¼çµæœå·²å­˜ç‚º 'inference_result.png'")
    print("è«‹æ‰“é–‹åœ–ç‰‡ï¼Œçœ‹çœ‹ç´…ç·š(çœŸå¯¦å€¼)æ˜¯å¦è½åœ¨è—ç·š(AIé æ¸¬)çš„é«˜å³°é™„è¿‘ï¼Ÿ")
    
